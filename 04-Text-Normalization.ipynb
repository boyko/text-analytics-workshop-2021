{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/amarov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we divide the articles into individual sentences by splitting the string by \".\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalisation\n",
    "\n",
    "### Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ballet_url = \"https://raw.githubusercontent.com/boyko/text-analytics-script/main/data/ballet.txt\"\n",
    "url_handle = request.urlopen(ballet_url)\n",
    "articles = url_handle.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['With apologies to James Brown, the hardest working people in show business may well be ballet dancers.', 'And at New York City Ballet, none work harder than the dancers in its lowest rank, the corps de ballet.', 'During the first week of the company’s winter season, Claire Kretzschmar, 24, a rising corps member, danced in all seven performances, appearing in five ballets, sometimes changing costumes at intermission to dance two roles in a night.', 'But her work onstage did not even begin to capture the stamina required to be in the corps.', 'Spending a week shadowing Ms. Kretzschmar was exhausting — she gave new meaning to the idea of being on your feet all day.', 'Twelve-hour days at the David H. Koch Theater, the company’s Lincoln Center home, were hardly unusual: Company class each morning was followed by back-to-back-to-back rehearsals, with occasional breaks for costume fittings or physical therapy, and then by the hair-makeup-costume-dance routine of daily performances.', 'Video\\nEvery day, Claire Kretzschmar of the New York City Ballet goes from class to rehearsal to performance at a breathless pace.', 'Dance alongside her busy day.', 'CREDIT PHOTOGRAPH BY SASHA ARUTYUNOVA FOR THE NEW YORK TIMES.', 'TECHNOLOGY BY SAMSUNG.']\n"
     ]
    }
   ],
   "source": [
    "naive_sentences = re.split(r\"[.!?] \", articles)\n",
    "sentences = nltk.sent_tokenize(articles)\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences from sent_tokenize: 91. Number of sentences from naive split: 116.\n"
     ]
    }
   ],
   "source": [
    "## Compare with naive split on \".\"\n",
    "print(\n",
    "    f\"Number of sentences from sent_tokenize: {len(sentences)}. Number of sentences from naive split: {len(naive_sentences)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Word tokenization\n",
    "\n",
    "Splitting a sentence into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = [nltk.word_tokenize(w) for w in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An alternative is to use [spacy](https://spacy.io/usage/linguistic-features#how-tokenizer-works)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "The basic independent unit in natural languages are the _morphemes_. These arise by inflecting\n",
    "a stem with some affix (e.g. prefixes, suffixes)\n",
    "\n",
    "- *Stemming*: find the stem of the word\n",
    "- *Lemmatization*: find the base form (lemma) of the word by removing affixes. The root form of the word must be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strang'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "lc_stemmer = LancasterStemmer()\n",
    "\n",
    "p_stemmer.stem(\"strange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strange'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_stemmer.stem(\"strange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce all words to their base form we can use a lemmatizer. The `nltk` package provides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the lemmatizer to work it requires the POS tags of the words in the sentence.\n",
    "The `WorldNetLemmatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_lemmatizer = nltk.WordNetLemmatizer()\n",
    "wn_lemmatizer.lemmatize('cars', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_lemmatizer = nltk.WordNetLemmatizer()\n",
    "wn_lemmatizer.lemmatize('running', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word is returned unchanged if no match is found. The default pos tag is \"n\" (noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gsdgfsdf'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_lemmatizer.lemmatize(\"gsdgfsdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging\n",
    "\n",
    "Part of speech (POS) tagging refers to determining the role of each word within a sentence, e.g.:\n",
    "\n",
    "1. CD  Cardinal number\n",
    "    e.g. 1, 20\n",
    "2. DT  Determiner\n",
    "    e.g. a/an, the, 2, some, many\n",
    "3. JJ  Adjective\n",
    "    e.g. good, big, red\n",
    "4. JJR Adjective, comparative\n",
    "5. MD  Modal\n",
    "    e.g. can, must, may, might, will, would, should\n",
    "6. NN  Noun, singular or mass\n",
    "    e.g. day, cat\n",
    "7. NNS Noun, plural\n",
    "    e.g. cats, dogs\n",
    "8. RB  Adverb\n",
    "    e.g. quickly, silently, well, badly, very, really\n",
    "9. RBR Adverb, comparative\n",
    "    e.g. cheaper/more cheaply, slower/more slowly\n",
    "10. RBS Adverb, superlative\n",
    "    e.g. fastest, hardest\n",
    "11. VB  Verb, base form\n",
    "    e.g. be, have\n",
    "12. WP  Wh-pronoun\n",
    "    e.g. I, you, he, she, some\n",
    "13. WP$ Possessive wh-pronoun\n",
    "    e.g. mine, his, her, your, yours\n",
    "\n",
    "for the full list of Penn Treebank POS tags see their [web site](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "Here we use the default POS tagger from `nltk` to label each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It ( PRP ) --> It\n",
      "is ( VBZ ) --> be\n",
      "better ( RBR ) --> well\n",
      "to ( TO ) --> to\n",
      "be ( VB ) --> be\n",
      "hated ( VBN ) --> hat\n",
      "for ( IN ) --> for\n",
      "what ( WP ) --> what\n",
      "you ( PRP ) --> you\n",
      "are ( VBP ) --> be\n",
      "than ( IN ) --> than\n",
      "to ( TO ) --> to\n",
      "be ( VB ) --> be\n",
      "loved ( VBN ) --> love\n",
      "for ( IN ) --> for\n",
      "what ( WP ) --> what\n",
      "you ( PRP ) --> you\n",
      "are ( VBP ) --> be\n",
      "not ( RB ) --> not\n",
      ". ( . ) --> .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda: wn.NOUN, J = wn.ADV, V = wn.VERB, R = wn.ADV)\n",
    "\n",
    "sentence = \"It is better to be hated for what you are than to be loved for what you are not.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "    print(token, \"(\", tag, \")\", \"-->\", lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'J': 'r',\n",
       "             'V': 'v',\n",
       "             'R': 'r',\n",
       "             'P': 'n',\n",
       "             'T': 'n',\n",
       "             'I': 'n',\n",
       "             'W': 'n',\n",
       "             '.': 'n'})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'be', 'well', 'to', 'be', 'hate', 'for', 'what', 'you', 'be', 'than', 'to', 'be', 'love', 'for', 'what', 'you', 'be', 'not', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Natural language contains a lot of words that appear very frequently in all text but that contribute little\n",
    "to the meaning of the text, for example: \"and\", \"the\", etc. Such words are commonly removed from the text in the process of normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_en = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to remove the stop words from a list of words is to simply use list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room', 'without', 'books', 'like', 'body', 'without', 'soul.', 'cicero.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"A room without books is like a body without a soul. Cicero.\".lower()\n",
    "list_of_words = words.split(\" \")\n",
    "\n",
    "list_of_words_filtered = list([w for w in list_of_words if w not in stopwords_en])\n",
    "list_of_words_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}